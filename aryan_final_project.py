# -*- coding: utf-8 -*-
"""Aryan_Final_project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DlitQVnJ13ZHyN1YLyZLm9xzexECvGg4
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pp
import numpy as nn
import matplotlib.pyplot as pl
import seaborn as ss
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report

rish = pp.read_csv("/content/drive/MyDrive/Amazon_Unlocked_Mobile.csv")
rish = rish.dropna(subset=['Rating', 'Reviews'])

rish = rish.dropna()

total_unique_brands = rish['Brand Name'].nunique()

print("Total number of unique brands:", total_unique_brands)

pl.figure(figsize=(12, 6))
ss.scatterplot(x='Rating', y='Price', hue='Brand Name', data=rish, legend=False)
pl.title('Rating vs Price')
pl.xlabel('Rating')
pl.ylabel('Price')
pl.show()

pl.figure(figsize=(12, 6))
ss.scatterplot(x='Rating', y='Price', hue='Brand Name', data=rish)
pl.title('Scatter Plot of Rating vs Price by Brand')
pl.xlabel('Rating')
pl.ylabel('Price')
pl.show()

pl.figure(figsize=(8, 6))
ss.histplot(rish['Rating'], bins=5, kde=True)
pl.title('Distribution of Ratings')
pl.xlabel('Rating')
pl.ylabel('Frequency')
pl.show()

avg_rating_by_brand = rish.groupby('Brand Name')['Rating'].mean().sort_values(ascending=False)
pl.figure(figsize=(10, 6))
avg_rating_by_brand.plot(kind='bar')
pl.title('Average Rating')
pl.xlabel('Brand')
pl.ylabel('Average Rating')
pl.show()

R_train, R_test, a_train, a_test = train_test_split(rish[['Reviews', 'Price']], rish['Rating'], test_size=0.2, random_state=42)

vz = TfidfVectorizer(stop_words='english', max_features=5000)
R_train_t = vz.fit_transform(R_train['Reviews'])
R_test_t = vz.transform(R_test['Reviews'])

R_train_com = pp.concat([pp.DataFrame(R_train_t.toarray()), R_train['Price'].reset_index(drop=True)], axis=1)# this is most important step for better prediction because here the all zero value is converted to dense array , so dense array may take average or max or min median. so baqsically dense array wwants non zero value and then using reset we are resetting th evalue or index
R_test_com= pp.concat([pp.DataFrame(R_test_t.toarray()), R_test['Price'].reset_index(drop=True)], axis=1)

R_train_com.columns = R_train_com.columns.astype(str)
R_test_com.columns = R_test_com.columns.astype(str) # converting all array to string

NB_classifier = MultinomialNB()
NB_classifier.fit(R_train_t, a_train)

RF_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
RF_classifier.fit(R_train_com, a_train)

NB_predict = NB_classifier.predict(R_test_t)
RF_predict = RF_classifier.predict(R_test_com)

acc_NB = accuracy_score(a_test,NB_predict)
print("NB Model Accuracy:", acc_NB)

cm = classification_report(a_test,NB_predict)
print(cm)

acc_RF = accuracy_score(a_test,RF_predict)
print("RF Model Accuracy:", acc_RF)

cm = classification_report(a_test,RF_predict)
print(cm)

hybrid_predict = (NB_predict + RF_predict) / 2

a_test_discrete = nn.round(a_test).astype(int)
rounded_hybrid_predict = nn.round(hybrid_predict).astype(int)  # Round to the nearest integer
accuracy = accuracy_score(a_test_discrete, rounded_hybrid_predict)

print("Hybrid Model Accuracy:", accuracy)

cm = classification_report(a_test_discrete, rounded_hybrid_predict)
print(cm)